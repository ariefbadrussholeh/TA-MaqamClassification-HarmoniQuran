{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b90c67-e78a-4ab5-b8d7-91ed6741e753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9b1371-07c3-4472-99ff-348fad1b1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 01:48:36.081150: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-13 01:48:36.913493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5f0e5e-336d-4052-8b83-959fd643d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X, feature):\n",
    "    mfcc = X[:, :, :20]\n",
    "    chroma = X[:, :, 21:33]\n",
    "    zcr = X[:, :, 20][:, :, np.newaxis]\n",
    "    rms = X[:, :, 33][:, :, np.newaxis]\n",
    "    centroid = X[:, :, 34][:, :, np.newaxis]\n",
    "    bandwidth = X[:, :, 35][:, :, np.newaxis]\n",
    "    rolloff = X[:, :, 36][:, :, np.newaxis]\n",
    "    \n",
    "    if feature == \"full\":\n",
    "        return X\n",
    "    elif feature == \"without_mfcc\":\n",
    "        return np.concatenate((zcr, chroma, rms, centroid, bandwidth, rolloff), axis=2)\n",
    "    elif feature == \"without_chroma\":\n",
    "        return np.concatenate((mfcc, zcr, rms, centroid, bandwidth, rolloff), axis=2)\n",
    "    elif feature == \"without_mfcc_and_chroma\":\n",
    "        return np.concatenate((zcr, rms, centroid, bandwidth, rolloff), axis=2)\n",
    "    elif feature == \"without_zcr\":\n",
    "        return np.concatenate((mfcc, chroma, rms, centroid, bandwidth, rolloff), axis=2)\n",
    "    elif feature == \"without_rms\":\n",
    "        return np.concatenate((mfcc, chroma, zcr, centroid, bandwidth, rolloff), axis=2)\n",
    "    elif feature == \"without_spec_cent\":\n",
    "        return np.concatenate((mfcc, chroma, zcr, rms, bandwidth, rolloff), axis=2)\n",
    "    elif feature == \"without_spec_band\":\n",
    "        return np.concatenate((mfcc, chroma, zcr, rms, centroid, rolloff), axis=2)\n",
    "    elif feature == \"without_spec_roll\":\n",
    "        return np.concatenate((mfcc, chroma, zcr, rms, centroid, bandwidth), axis=2)\n",
    "    elif feature == \"mfcc\":\n",
    "        return mfcc\n",
    "    elif feature == \"chroma\":\n",
    "        return chroma\n",
    "    elif feature == \"mfcc_and_chroma\":\n",
    "        return np.concatenate((mfcc, chroma), axis=2)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature option\")\n",
    "\n",
    "def extract_features_all(feature, X_train, X_val, y_train, y_val):\n",
    "    return extract_features(X_train, feature), extract_features(X_train_val, feature), extract_features(X_test, feature), extract_features(X_val, feature)\n",
    "\n",
    "def log(log_data):\n",
    "    log_file = \"Log/Log.csv\"\n",
    "    df = pd.DataFrame([log_data])\n",
    "    if os.path.exists(log_file):\n",
    "        df.to_csv(log_file, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(log_file, index=False)\n",
    "\n",
    "def format_seconds_to_hhmmss(seconds):\n",
    "    # Convert seconds to a timedelta object\n",
    "    delta = datetime.timedelta(seconds=seconds)\n",
    "    \n",
    "    # Get the total hours, minutes, and seconds\n",
    "    hours, remainder = divmod(delta.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    formatted_time = f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "    return formatted_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906291de-a120-48de-8d53-55eea71396a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(title, create_model, optimizer, lr, batch, X_train, X_train_val, X_val, X_test):\n",
    "    print(f\"-> {title}\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "    fold_losses = []\n",
    "    fold_f1_scores = []\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X_train_val, y_train_val), start=1):\n",
    "      print(f\"Fold-{i}\")\n",
    "      X_train_fold, X_test_fold = X_train_val[train_index], X_train_val[val_index]\n",
    "      y_train_fold, y_test_fold = y_train_val[train_index], y_train_val[val_index]\n",
    "      model = create_model(X_train_fold)\n",
    "      optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "      if optimizer == \"Adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "      elif optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "      elif optimizer == \"RMSprop\":\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=lr)      \n",
    "      model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "      early_stopping = keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)\n",
    "      history = model.fit(X_train_fold, y_train_fold, epochs=200, batch_size=batch, callbacks=[early_stopping, TqdmCallback(verbose=0)], verbose=0)\n",
    "      loss, acc = model.evaluate(X_test_fold, y_test_fold)\n",
    "      y_pred_fold = np.argmax(model.predict(X_test_fold), axis=1)\n",
    "      f1 = f1_score(y_test_fold, y_pred_fold, average='weighted')\n",
    "      fold_accuracies.append(acc)\n",
    "      fold_losses.append(loss)\n",
    "      fold_f1_scores.append(f1)\n",
    "    \n",
    "    model = create_model(X_train)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    if optimizer == \"Adam\":\n",
    "      optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == \"SGD\":\n",
    "      optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "      optimizer = keras.optimizers.RMSprop(learning_rate=lr)      \n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=batch, validation_data=(X_val, y_val), callbacks=[early_stopping, TqdmCallback(verbose=0)], verbose=0)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Extract the data\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(train_accuracy) + 1)\n",
    "    last_epoch = len(train_accuracy)\n",
    "    last_train_acc = train_accuracy[-1]\n",
    "    last_val_acc = val_accuracy[-1]\n",
    "    last_train_loss = train_loss[-1]\n",
    "    last_val_loss = val_loss[-1]\n",
    "    training_time = format_seconds_to_hhmmss(end_time - start_time)\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.plot(epochs, train_accuracy, label=f'train_accuracy (last: {last_train_acc:.3f})')\n",
    "    plt.plot(epochs, val_accuracy, label=f'val_accuracy (last: {last_val_acc:.3f})')\n",
    "    plt.title(f'Model Accuracy (Epochs: {last_epoch})')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"Log/{title}_model_accuracy.png\")\n",
    "    plt.clf()\n",
    "    plt.plot(epochs, train_loss, label=f'train_loss (last: {last_train_loss:.3f})')\n",
    "    plt.plot(epochs, val_loss, label=f'val_loss (last: {last_val_loss:.3f})')\n",
    "    plt.title(f'Model Accuracy (Epochs: {last_epoch})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"Log/{title}_model_loss.png\")\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    maqams = [\"Bayati\", \"Hijaz\", \"Jiharkah\", \"Nahawand\", \"Rast\", \"Saba\", \"Sikah\"]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=maqams, yticklabels=maqams)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig(f\"Log/{title}_confussion_matrix.png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    log_data = {\n",
    "        \"title\": title,\n",
    "        \"epoch_stopped\": last_epoch,\n",
    "        \"training_time\": training_time,\n",
    "        \"train_acc\": last_train_acc,\n",
    "        \"train_loss\": last_train_loss,\n",
    "        \"val_acc\": last_val_acc,\n",
    "        \"val_loss\": last_val_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_f1_score\": f1,\n",
    "        \"fold_mean_acc\": np.mean(fold_accuracies),\n",
    "        \"fold_mean_loss\": np.mean(fold_losses),\n",
    "        \"fold_mean_f1_score\": np.mean(fold_f1_scores)\n",
    "    }\n",
    "    for fold in range(5):\n",
    "        log_data[f\"fold_{fold+1}_acc\"] = fold_accuracies[fold]\n",
    "        log_data[f\"fold_{fold+1}_loss\"] = fold_losses[fold]\n",
    "        log_data[f\"fold_{fold+1}_f1_score\"] = fold_f1_scores[fold]\n",
    "    log(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c180d5-b6cc-4022-97b8-c17520b2ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(\"Dataset/Mujawwad1_(hop=5).npz\")\n",
    "\n",
    "features = dataset[\"features\"]\n",
    "labels = dataset[\"labels\"]\n",
    "\n",
    "X = features\n",
    "y = labels\n",
    "\n",
    "mean = np.mean(X, axis=(0, 1))\n",
    "std = np.std(X, axis=(0, 1))\n",
    "X = (X - mean) / std\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, stratify=y_train_val)\n",
    "\n",
    "X_train_temp = X_train\n",
    "X_train_val_temp = X_train_val\n",
    "X_val_temp = X_val\n",
    "X_test_temp = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c8d02-15e0-4fd5-8047-b81427dacb2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> feature=full\n",
      "Fold-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 01:48:47.696560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2835 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 7g.40gb, pci bus id: 0000:4e:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d61a9058f84ad29609b77886781bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720810129.540901  561827 service.cc:145] XLA service 0x7f959c003b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1720810129.540964  561827 service.cc:153]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 7g.40gb, Compute Capability 8.0\n",
      "2024-07-13 01:48:49.643125: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-13 01:48:51.410780: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1720810135.408605  561827 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9696 - loss: 0.1264 \n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "Fold-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caf21067e6b44c0abb44589f9d69858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9754 - loss: 0.0634\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Fold-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebdccb7415341208296ff917da88a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9659 - loss: 0.1708\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Fold-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23d1ab9918e4603b77a52ad45776d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9800 - loss: 0.0784\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Fold-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1986daeee791453c8f37dc94a9b043e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9628 - loss: 0.1194\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4d3245a79f4dedb65d91248445d3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9786 - loss: 0.0938\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "-> feature=without_mfcc\n",
      "Fold-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf98cfd68d7420088885c7965976343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9361 - loss: 0.1804\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Fold-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad00e1655fb84f118aa5710e46860f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9641 - loss: 0.1171\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "Fold-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084d0f338bec48d2983e2abf15fdc79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9506 - loss: 0.1689\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"full\", \"without_mfcc\", \"without_chroma\", \"without_mfcc_and_chroma\",\n",
    "    \"without_zcr\", \"without_rms\", \"without_spec_cent\", \"without_spec_band\",\n",
    "    \"without_spec_roll\", \"mfcc\", \"chroma\", \"mfcc_and_chroma\"\n",
    "]\n",
    "\n",
    "for feature in features:\n",
    "    X_train = X_train_temp\n",
    "    X_train_val = X_train_val_temp\n",
    "    X_val = X_val_temp\n",
    "    X_test = X_test_temp\n",
    "\n",
    "    X_train, X_train_val, X_test, X_val = extract_features_all(feature, X_train, X_val, y_train, y_val)\n",
    "\n",
    "    X_train = np.mean(X_train, axis=1)\n",
    "    X_train_val = np.mean(X_train_val, axis=1)\n",
    "    X_test = np.mean(X_test, axis=1)\n",
    "    X_val = np.mean(X_val, axis=1)\n",
    "\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_train_val = X_train_val[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_val = X_val[..., np.newaxis]\n",
    "\n",
    "    train(f\"feature={feature}\", Model.deep_ann_1d, \"Adam\", 0.0001, 64, X_train, X_train_val, X_val, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
